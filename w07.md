---
permalink: /W07/
---
[HOME](../)

<br>
# Top 10 List of Week 07

1. Critical section<br>
Critical section contains shared variables or resources which are needed to be synchronized to maintain consistency of data variable.<br>
In simple terms a critical section is group of instructions/statements or region of code that need to be executed atomically (read this post for atomicity), such as accessing a resource (file, input or output port, global data, etc.). <br>

2. Semaphore<br>
Semaphore was proposed by Dijkstra in 1965 which is a very significant technique to manage concurrent processes by using a simple integer value, which is known as a semaphore.<br>
Semaphore is simply a variable which is non-negative and shared between threads. This variable is used to solve the critical section problem and to achieve process synchronization in the multiprocessing environment. 

3. Peterson’s Algorithm<br>
Peterson’s Algorithm is used to synchronize two processes. It uses two variables, a bool array flag of size 2 and an int variable turn to accomplish it.<br>
In the solution i represents the Consumer and j represents the Producer. Initially the flags are false. When a process wants to execute it’s critical section, it sets it’s flag to true and turn as the index of the other process. This means that the process wants to execute but it will allow the other process to run first. The process performs busy waiting until the other process has finished it’s own critical section.<br>
After this the current process enters it’s critical section and adds or removes a random number from the shared buffer. After completing the critical section, it sets it’s own flag to false, indication it does not wish to execute anymore.

4. Starvation<br>
Starvation is the problem that occurs when high priority processes keep executing and low priority processes get blocked for indefinite time. In heavily loaded computer system, a steady stream of higher-priority processes can prevent a low-priority process from ever getting the CPU. In starvation resources are continuously utilized by high priority processes. Problem of starvation can be resolved using Aging. In Aging priority of long waiting processes is gradually increased.

5. Deadlock Prevention<br>
We can prevent Deadlock by eliminating any of the above four conditions : Eliminate Mutual Exclusion, Eliminate Hold and Wait, Eliminate No Preemption, and Eliminate Circular Wait.

6. Deadlock Avoidance<br>
Deadlock avoidance can be done with Banker’s Algorithm.<br>

7. Bankers’s Algorithm<br>
Bankers’s Algorithm is resource allocation and deadlock avoidance algorithm which test all the request made by processes for resources, it checks for the safe state, if after granting request system remains in the safe state it allows the request and if there is no safe state it doesn’t allow the request made by the process.

8. Shared Memory Model<br>
In this IPC model, a shared memory region is established which is used by the processes for data communication. This memory region is present in the address space of the process which creates the shared memory segment.The processes who want to communicate with this process should attach this memory segment into their address space.


9. Message Passing Model<br>
In this model, the processes communicate with each other by exchanging messages. For this purpose a communication link must exist between the processes and it must facilitate at least two operations send (message) and receive (message). Size of messages may be variable or fixed.

10. Race condition<br>
A race condition is an undesirable situation that occurs when two or more processes executing in a system with an illusion of concurrency and accessing shared data, may try to change the shared data at the same time. Since the process scheduling algorithm can swap between processes at any time, we don't actually know the order in which the processes will access the data leading to inconsistencies in the system.
